{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eb9cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "numpy_path = '/Users/shaikimran/Documents/Text Analytics/.venv/lib/python3.10/site-packages/numpy'\n",
    "numpy_dist_info = '/Users/shaikimran/Documents/Text Analytics/.venv/lib/python3.10/site-packages/numpy-1.26.4.dist-info'\n",
    "\n",
    "# Remove directories if they exist\n",
    "for path in [numpy_path, numpy_dist_info]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Removed: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50cbe883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: /Users/shaikimran/Documents/Text Analytics/.venv/bin/python\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install transformers deepface tf-keras CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a018596f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (2.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b7f884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaikimran/Documents/Text Analytics/.venv/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# üé® AGE-CONDITIONED FORENSIC SKETCH GENERATOR\n",
    "# PyTorch Version - Kaggle Compatible\n",
    "# =========================================================\n",
    "\n",
    "import os, time, json, torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad955c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, uninstall existing tensorflow and install Apple Silicon version\n",
    "%pip uninstall tensorflow tensorflow-macos -y\n",
    "%pip install tensorflow-macos\n",
    "%pip install tensorflow-metal  # For GPU acceleration on Apple Silicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d17e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üßπ CLEAN UP CURRENT ENVIRONMENT\n",
    "# =========================================================\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"üßπ Cleaning up environment...\")\n",
    "\n",
    "# Remove problematic packages\n",
    "packages_to_remove = ['tensorflow', 'tensorflow-macos', 'tensorflow-metal', 'deepface']\n",
    "for package in packages_to_remove:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'uninstall', '-y', package])\n",
    "        print(f\"‚úÖ Removed {package}\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è Could not remove {package}\")\n",
    "\n",
    "print(\"Environment cleanup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üöÄ CLEAN INSTALLATION (After Kernel Restart)\n",
    "# =========================================================\n",
    "import os\n",
    "# CRITICAL: Disable TensorFlow completely\n",
    "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['NO_TF'] = '1'\n",
    "\n",
    "print(\"üì¶ Installing clean packages...\")\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install only what we need, avoiding TensorFlow dependencies\n",
    "packages = [\n",
    "    'transformers>=4.21.0',\n",
    "    'torch',\n",
    "    'torchvision',\n",
    "    'pillow',\n",
    "    'matplotlib',\n",
    "    'tqdm',\n",
    "    'numpy',\n",
    "    'opencv-python',\n",
    "    'git+https://github.com/openai/CLIP.git'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to install {package}: {e}\")\n",
    "\n",
    "print(\"‚úÖ Package installation completed!\")\n",
    "\n",
    "# Now import\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import transformers - this should work now\n",
    "try:\n",
    "    from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "    print(\"‚úÖ BLIP loaded successfully!\")\n",
    "    \n",
    "    # Test that it works\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    print(\"‚úÖ BLIP models loaded and working!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå BLIP import failed: {e}\")\n",
    "    BlipProcessor, BlipForConditionalGeneration = None, None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå BLIP model loading failed: {e}\")\n",
    "    BlipProcessor, BlipForConditionalGeneration = None, None\n",
    "\n",
    "# Import CLIP\n",
    "try:\n",
    "    import clip\n",
    "    print(\"‚úÖ CLIP loaded successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå CLIP import failed: {e}\")\n",
    "    clip = None\n",
    "\n",
    "# Skip DeepFace completely\n",
    "print(\"‚ö†Ô∏è DeepFace skipped to avoid TensorFlow issues\")\n",
    "DeepFace = None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b858922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üîß DEVICE SETUP\n",
    "# =========================================================\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"\\n‚úÖ Using device: {device}\")\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c999af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# üìÅ Configuration\n",
    "# =========================================================\n",
    "\n",
    "# Local macOS path\n",
    "dataset_path = \"/Users/shaikimran/Documents/Text Analytics/archive\"\n",
    "train_A = os.path.join(dataset_path, \"photo\")\n",
    "train_B = os.path.join(dataset_path, \"cropped_sketch\")\n",
    "\n",
    "# Model hyperparameters\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 8\n",
    "LAMBDA_L1 = 100\n",
    "EPOCHS = 10\n",
    "TEXT_EMB_DIM = 512\n",
    "AGE_EMB_DIM = 64\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Create output directories (for local storage)\n",
    "os.makedirs(\"./checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"./age_progression_results\", exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Dataset paths:\")\n",
    "print(f\"  Photos: {train_A}\")\n",
    "print(f\"  Exists: {os.path.exists(train_A)}\")\n",
    "print(f\"  Sketches: {train_B}\")\n",
    "print(f\"  Exists: {os.path.exists(train_B)}\")\n",
    "\n",
    "# Verify dataset structure\n",
    "if os.path.exists(train_A):\n",
    "    photo_files = [f for f in os.listdir(train_A) if f.endswith(('.jpg', '.png', '.jpeg', '.JPG', '.PNG', '.JPEG'))]\n",
    "    photo_count = len(photo_files)\n",
    "    print(f\"\\nüì∏ Found {photo_count} photos\")\n",
    "    if photo_count > 0:\n",
    "        print(f\"  Sample files: {photo_files[:3]}\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è Photo directory not found!\")\n",
    "\n",
    "if os.path.exists(train_B):\n",
    "    sketch_files = [f for f in os.listdir(train_B) if f.endswith(('.jpg', '.png', '.jpeg', '.JPG', '.PNG', '.JPEG'))]\n",
    "    sketch_count = len(sketch_files)\n",
    "    print(f\"\\n‚úèÔ∏è Found {sketch_count} sketches\")\n",
    "    if sketch_count > 0:\n",
    "        print(f\"  Sample files: {sketch_files[:3]}\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è Sketch directory not found!\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Model Configuration:\")\n",
    "print(f\"  Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Lambda L1: {LAMBDA_L1}\")\n",
    "print(f\"  Text Embedding Dim: {TEXT_EMB_DIM}\")\n",
    "print(f\"  Age Embedding Dim: {AGE_EMB_DIM}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"\\nüíæ Output directories:\")\n",
    "print(f\"  Checkpoints: ./checkpoints\")\n",
    "print(f\"  Results: ./age_progression_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b4b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üéØ Load CLIP Model (with fallback)\n",
    "# =========================================================\n",
    "clip_model = None\n",
    "clip_preprocess = None\n",
    "\n",
    "if clip is not None:\n",
    "    try:\n",
    "        print(\"\\nüîÑ Loading CLIP model...\")\n",
    "        clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "        print(\"‚úÖ CLIP model loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è CLIP loading failed: {e}\")\n",
    "        clip_model = None\n",
    "\n",
    "def clip_text_embedding(text):\n",
    "    \"\"\"Get CLIP text embedding with fallback - MPS COMPATIBLE\"\"\"\n",
    "    if clip_model is None:\n",
    "        # Fallback: simple hash-based embedding\n",
    "        np.random.seed(hash(text) % (2**32))\n",
    "        return np.random.randn(512).astype(np.float32)\n",
    "    \n",
    "    try:\n",
    "        tokens = clip.tokenize([text]).to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.encode_text(tokens)\n",
    "        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "        # Ensure float32 before converting to numpy\n",
    "        return embedding.cpu().float().numpy().flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Text embedding error: {e}\")\n",
    "        np.random.seed(hash(text) % (2**32))\n",
    "        return np.random.randn(512).astype(np.float32)\n",
    "\n",
    "def clip_image_embedding(image_tensor):\n",
    "    \"\"\"Get CLIP image embedding with fallback - MPS COMPATIBLE\"\"\"\n",
    "    if clip_model is None or clip_preprocess is None:\n",
    "        return np.random.randn(512).astype(np.float32)\n",
    "    \n",
    "    try:\n",
    "        if isinstance(image_tensor, np.ndarray):\n",
    "            image_tensor = Image.fromarray(image_tensor.astype(np.uint8))\n",
    "        \n",
    "        img_preprocessed = clip_preprocess(image_tensor).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.encode_image(img_preprocessed)\n",
    "        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "        # Ensure float32 before converting to numpy\n",
    "        return embedding.cpu().float().numpy().flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Image embedding error: {e}\")\n",
    "        return np.random.randn(512).astype(np.float32)\n",
    "\n",
    "def clip_similarity(text, image_array):\n",
    "    \"\"\"Calculate CLIP similarity with fallback - MPS COMPATIBLE\"\"\"\n",
    "    try:\n",
    "        text_emb = clip_text_embedding(text)\n",
    "        img_emb = clip_image_embedding(image_array)\n",
    "        \n",
    "        # L2 normalization (all float32)\n",
    "        text_norm = np.linalg.norm(text_emb)\n",
    "        img_norm = np.linalg.norm(img_emb)\n",
    "        \n",
    "        if text_norm > 0:\n",
    "            text_emb = text_emb / text_norm\n",
    "        if img_norm > 0:\n",
    "            img_emb = img_emb / img_norm\n",
    "        \n",
    "        # Cosine similarity (should be between -1 and 1)\n",
    "        similarity = float(np.dot(text_emb, img_emb))\n",
    "        \n",
    "        # Clip to valid range\n",
    "        similarity = np.clip(similarity, -1.0, 1.0)\n",
    "        \n",
    "        return float(similarity)  # Ensure it's a Python float, not numpy.float64\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è CLIP similarity error: {e}\")\n",
    "        return 0.0  # Neutral similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8359a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üë§ DeepFace Analysis (with fallback)\n",
    "# =========================================================\n",
    "def analyze_face(image_path):\n",
    "    \"\"\"Analyze face with DeepFace (with fallback)\"\"\"\n",
    "    if DeepFace is None:\n",
    "        return {'age': 30, 'gender': 'Man', 'race': 'white', 'emotion': 'neutral'}\n",
    "    \n",
    "    try:\n",
    "        analysis = DeepFace.analyze(\n",
    "            img_path=image_path,\n",
    "            actions=['age', 'gender', 'race', 'emotion'],\n",
    "            enforce_detection=False,\n",
    "            silent=True\n",
    "        )\n",
    "        if isinstance(analysis, list):\n",
    "            analysis = analysis[0]\n",
    "        return {\n",
    "            'age': int(analysis['age']),\n",
    "            'gender': analysis.get('dominant_gender', 'Man'),\n",
    "            'race': analysis.get('dominant_race', 'white'),\n",
    "            'emotion': analysis.get('dominant_emotion', 'neutral')\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Face analysis failed: {e}\")\n",
    "        return {'age': 30, 'gender': 'Man', 'race': 'white', 'emotion': 'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f397f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# ‚è∞ Age Encoding\n",
    "# =========================================================\n",
    "def parse_temporal_description(description):\n",
    "    \"\"\"Parse temporal info from text\"\"\"\n",
    "    import re\n",
    "    \n",
    "    years_ago_match = re.search(r'(\\d+)\\s+years?\\s+ago', description.lower())\n",
    "    age_match = re.search(r'(?:about|around|aged?)?\\s*(\\d+)(?:\\s*years?\\s*old)?', description.lower())\n",
    "    \n",
    "    current_age = None\n",
    "    years_elapsed = 0\n",
    "    \n",
    "    if age_match:\n",
    "        current_age = int(age_match.group(1))\n",
    "    if years_ago_match:\n",
    "        years_elapsed = int(years_ago_match.group(1))\n",
    "    \n",
    "    if 'elderly' in description.lower() and not current_age:\n",
    "        current_age = 70\n",
    "    elif 'middle-aged' in description.lower() and not current_age:\n",
    "        current_age = 45\n",
    "    elif 'young' in description.lower() and not current_age:\n",
    "        current_age = 25\n",
    "    \n",
    "    target_age = (current_age or 30) + years_elapsed\n",
    "    \n",
    "    return {\n",
    "        'described_age': current_age or 30,\n",
    "        'years_ago': years_elapsed,\n",
    "        'target_age': target_age,\n",
    "        'raw_description': description\n",
    "    }\n",
    "\n",
    "def encode_age(age):\n",
    "    \"\"\"Encode age to vector\"\"\"\n",
    "    age_normalized = np.clip(age / 100.0, 0, 1)\n",
    "    age_vector = np.array([\n",
    "        age_normalized,\n",
    "        np.sin(age_normalized * np.pi),\n",
    "        np.cos(age_normalized * np.pi),\n",
    "        age_normalized ** 2,\n",
    "        np.sqrt(age_normalized),\n",
    "    ], dtype=np.float32)\n",
    "    age_embedding = np.tile(age_vector, AGE_EMB_DIM // len(age_vector) + 1)[:AGE_EMB_DIM]\n",
    "    return torch.FloatTensor(age_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üì¶ Dataset Class\n",
    "# =========================================================\n",
    "class SketchDataset(Dataset):\n",
    "    def __init__(self, photo_dir, sketch_dir, descriptions, ages):\n",
    "        self.photo_files = sorted([os.path.join(photo_dir, f) for f in os.listdir(photo_dir) \n",
    "                                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        self.sketch_files = sorted([os.path.join(sketch_dir, f) for f in os.listdir(sketch_dir) \n",
    "                                    if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        self.descriptions = descriptions\n",
    "        self.ages = ages\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(len(self.photo_files), len(self.sketch_files))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        photo_path = self.photo_files[idx]\n",
    "        sketch_path = self.sketch_files[idx]\n",
    "        filename = os.path.basename(photo_path)\n",
    "        \n",
    "        photo = Image.open(photo_path).convert('RGB')\n",
    "        sketch = Image.open(sketch_path).convert('RGB')\n",
    "        \n",
    "        photo = self.transform(photo)\n",
    "        sketch = self.transform(sketch)\n",
    "        \n",
    "        description = self.descriptions.get(filename, \"a person's face\")\n",
    "        age = self.ages.get(filename, 30)\n",
    "        \n",
    "        return photo, sketch, description, age, filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# =========================================================\n",
    "# üî§ FIXED CAPTION GENERATION - macOS M1 Compatible\n",
    "# =========================================================\n",
    "print(\"\\nüî§ Generating captions...\")\n",
    "\n",
    "image_files = [f for f in os.listdir(train_A) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "desc = {}\n",
    "face_attributes = {}\n",
    "estimated_ages = {}\n",
    "\n",
    "DETAILED_COUNT = 50  # Process first 50 images with BLIP\n",
    "\n",
    "# =========================================================\n",
    "# üöÄ FIXED BLIP LOADING - macOS M1 Compatible\n",
    "# =========================================================\n",
    "blip_processor = None\n",
    "blip_model = None\n",
    "\n",
    "try:\n",
    "    from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "    \n",
    "    print(\"üì¶ Loading BLIP model...\")\n",
    "    \n",
    "    # ‚úÖ FIX 1: Use torch.cuda instead of tf.config\n",
    "    # ‚úÖ FIX 2: Check for MPS (Apple Silicon GPU) support\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = \"mps\"  # Apple Silicon GPU\n",
    "        print(\"üçé Using Apple Silicon GPU (MPS)\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = \"cuda\"  # NVIDIA GPU\n",
    "        print(\"üéÆ Using CUDA GPU\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"üíª Using CPU\")\n",
    "    \n",
    "    # Load processor and model\n",
    "    blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip-image-captioning-base\"\n",
    "    )\n",
    "    \n",
    "    # ‚úÖ FIX 3: Move model to device AFTER loading\n",
    "    blip_model = blip_model.to(device)\n",
    "    blip_model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(f\"‚úÖ BLIP model loaded successfully on {device}\")\n",
    "    print(f\"   This will create UNIQUE captions for each face!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå BLIP import failed: {e}\")\n",
    "    print(\"   Install with: pip install transformers\")\n",
    "    blip_processor = None\n",
    "    blip_model = None\n",
    "    device = \"cpu\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è BLIP loading failed: {e}\")\n",
    "    print(\"   Continuing with basic captions...\")\n",
    "    blip_processor = None\n",
    "    blip_model = None\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# üé® PROCESS IMAGES WITH BLIP\n",
    "# =========================================================\n",
    "print(f\"\\nüñºÔ∏è Processing {len(image_files)} images...\")\n",
    "print(f\"   Detailed BLIP captions: {min(DETAILED_COUNT, len(image_files))} images\")\n",
    "print(f\"   Simple captions: {max(0, len(image_files) - DETAILED_COUNT)} images\")\n",
    "\n",
    "for idx, name in enumerate(tqdm(image_files, desc=\"Analyzing images\")):\n",
    "    try:\n",
    "        img_path = os.path.join(train_A, name)\n",
    "        \n",
    "        if idx < DETAILED_COUNT:\n",
    "            # =============================================\n",
    "            # Process with BLIP for detailed captions\n",
    "            # =============================================\n",
    "            \n",
    "            # Try face analysis (with fallback)\n",
    "            try:\n",
    "                attrs = analyze_face(img_path)\n",
    "            except:\n",
    "                attrs = {'age': 30, 'gender': 'unknown', 'race': 'unknown'}\n",
    "            \n",
    "            # Generate caption with BLIP\n",
    "            caption = \"a person's face\"  # Fallback caption\n",
    "            \n",
    "            if blip_processor is not None and blip_model is not None:\n",
    "                try:\n",
    "                    # Load and preprocess image\n",
    "                    img = Image.open(img_path).convert(\"RGB\")\n",
    "                    inputs = blip_processor(images=img, return_tensors=\"pt\")\n",
    "                    \n",
    "                    # ‚úÖ FIX 4: Move inputs to same device as model\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    # Generate caption\n",
    "                    with torch.no_grad():  # No gradient needed for inference\n",
    "                        output_ids = blip_model.generate(**inputs, max_length=50)\n",
    "                    \n",
    "                    # Decode caption\n",
    "                    caption = blip_processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # If BLIP fails for this image, use fallback\n",
    "                    print(f\"\\n   ‚ö†Ô∏è Caption generation failed for {name}: {e}\")\n",
    "                    caption = \"a person's face\"\n",
    "            \n",
    "            # Create detailed description\n",
    "            description = f\"{caption}, approximately {attrs['age']} years old, {attrs['gender']}, {attrs['race']} appearance\"\n",
    "            estimated_ages[name] = attrs['age']\n",
    "            desc[name] = description\n",
    "            face_attributes[name] = attrs\n",
    "            \n",
    "        else:\n",
    "            # =============================================\n",
    "            # Use simple description for remaining images\n",
    "            # =============================================\n",
    "            desc[name] = \"a person's face\"\n",
    "            estimated_ages[name] = 30\n",
    "            face_attributes[name] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Error processing {name}: {e}\")\n",
    "        desc[name] = \"a person's face\"\n",
    "        estimated_ages[name] = 30\n",
    "        face_attributes[name] = None\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# üíæ SAVE RESULTS\n",
    "# =========================================================\n",
    "os.makedirs(\"./outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"./outputs/descriptions.json\", \"w\") as f:\n",
    "    json.dump(desc, f, indent=2)\n",
    "\n",
    "with open(\"./outputs/estimated_ages.json\", \"w\") as f:\n",
    "    json.dump(estimated_ages, f, indent=2)\n",
    "\n",
    "with open(\"./outputs/face_attributes.json\", \"w\") as f:\n",
    "    # Convert None values to string for JSON serialization\n",
    "    face_attrs_serializable = {k: v if v is not None else \"none\" for k, v in face_attributes.items()}\n",
    "    json.dump(face_attrs_serializable, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Processing complete!\")\n",
    "print(f\"   Total images: {len(image_files)}\")\n",
    "print(f\"   Detailed BLIP analysis: {min(DETAILED_COUNT, len(image_files))}\")\n",
    "print(f\"   Descriptions saved: {len(desc)}\")\n",
    "print(f\"   Output location: ./outputs/\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# üìä DISPLAY SAMPLE RESULTS\n",
    "# =========================================================\n",
    "print(\"\\nüìä Sample descriptions:\")\n",
    "for i, (name, description) in enumerate(list(desc.items())[:5]):\n",
    "    print(f\"\\n{i+1}. {name}\")\n",
    "    print(f\"   {description}\")\n",
    "    print(f\"   Estimated age: {estimated_ages[name]}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# üîç VERIFY CAPTION DIVERSITY\n",
    "# =========================================================\n",
    "print(\"\\nüîç Caption Diversity Check:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract just the BLIP captions (before age/gender added)\n",
    "blip_captions = []\n",
    "for name, full_desc in list(desc.items())[:DETAILED_COUNT]:\n",
    "    # Extract the caption part (before the comma)\n",
    "    caption_part = full_desc.split(',')[0] if ',' in full_desc else full_desc\n",
    "    blip_captions.append(caption_part)\n",
    "\n",
    "# Check uniqueness\n",
    "unique_captions = set(blip_captions)\n",
    "print(f\"Total BLIP captions: {len(blip_captions)}\")\n",
    "print(f\"Unique captions: {len(unique_captions)}\")\n",
    "print(f\"Diversity: {len(unique_captions)/len(blip_captions)*100:.1f}%\")\n",
    "\n",
    "if len(unique_captions) < len(blip_captions) * 0.5:\n",
    "    print(\"‚ö†Ô∏è WARNING: Low caption diversity! BLIP may not be working properly.\")\n",
    "    print(\"   Expected: Each face should have a different description\")\n",
    "else:\n",
    "    print(\"‚úÖ Good caption diversity! Each face has unique features.\")\n",
    "\n",
    "print(\"\\nSample unique captions:\")\n",
    "for i, caption in enumerate(list(unique_captions)[:10]):\n",
    "    print(f\"   {i+1}. {caption}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# üí° NEXT STEPS\n",
    "# =========================================================\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if blip_processor is not None and blip_model is not None:\n",
    "    print(\"‚úÖ BLIP is working! Your captions are diverse.\")\n",
    "    print(\"   Now you can:\")\n",
    "    print(\"   1. Retrain the model with these diverse captions\")\n",
    "    print(\"   2. Train for 50-100 epochs\")\n",
    "    print(\"   3. The model will learn to generate different faces for different descriptions\")\n",
    "else:\n",
    "    print(\"‚ùå BLIP failed to load.\")\n",
    "    print(\"   Fix this by:\")\n",
    "    print(\"   1. Install transformers: pip install transformers\")\n",
    "    print(\"   2. Ensure you have a stable internet connection (to download BLIP)\")\n",
    "    print(\"   3. Check you have enough RAM (BLIP needs ~2GB)\")\n",
    "\n",
    "print(\"\\nüìù To use these captions for training:\")\n",
    "print(\"   dataset = SketchDataset(train_A, train_B, desc, estimated_ages)\")\n",
    "print(\"   dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üèóÔ∏è Generator Architecture (PyTorch)\n",
    "# =========================================================\n",
    "class AgeConditionedGenerator(nn.Module):\n",
    "    def __init__(self, text_dim=512, age_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial projection\n",
    "        self.fc1 = nn.Linear(text_dim + age_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 8 * 8 * 512)\n",
    "        \n",
    "        # Age modulation\n",
    "        self.age_mod = nn.Linear(age_dim, 512)\n",
    "        \n",
    "        # Upsampling blocks\n",
    "        self.up1 = self._upsample_block(512, 512, dropout=True)\n",
    "        self.up2 = self._upsample_block(512, 512, dropout=True)\n",
    "        self.up3 = self._upsample_block(512, 512, dropout=True)\n",
    "        self.up4 = self._upsample_block(512, 256)\n",
    "        self.up5 = self._upsample_block(256, 128)\n",
    "        \n",
    "        # Age conditioning layers\n",
    "        self.age_scale1 = nn.Linear(age_dim, 512)\n",
    "        self.age_scale2 = nn.Linear(age_dim, 512)\n",
    "        self.age_scale3 = nn.Linear(age_dim, 512)\n",
    "        \n",
    "        # Final conv\n",
    "        self.final = nn.Conv2d(128, 3, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def _upsample_block(self, in_ch, out_ch, dropout=False):\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout2d(0.5))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, text_emb, age_emb):\n",
    "        # Combine embeddings\n",
    "        x = torch.cat([text_emb, age_emb], dim=1)\n",
    "        \n",
    "        # Project\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = x.view(-1, 512, 8, 8)\n",
    "        \n",
    "        # Age modulation\n",
    "        age_mod = self.sigmoid(self.age_mod(age_emb)).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = x * age_mod\n",
    "        \n",
    "        # Upsample with age injection\n",
    "        x = self.up1(x)\n",
    "        age_s1 = self.sigmoid(self.age_scale1(age_emb)).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = x * age_s1\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        age_s2 = self.sigmoid(self.age_scale2(age_emb)).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = x * age_s2\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        age_s3 = self.sigmoid(self.age_scale3(age_emb)).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = x * age_s3\n",
    "        \n",
    "        x = self.up4(x)\n",
    "        x = self.up5(x)\n",
    "        \n",
    "        x = self.final(x)\n",
    "        return self.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üèóÔ∏è Discriminator Architecture\n",
    "# =========================================================\n",
    "class AgeAwareDiscriminator(nn.Module):\n",
    "    def __init__(self, age_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = self._conv_block(3, 64, normalize=False)\n",
    "        self.conv2 = self._conv_block(64, 128)\n",
    "        self.conv3 = self._conv_block(128, 256)\n",
    "        self.conv4 = self._conv_block(256, 512)\n",
    "        \n",
    "        # Age processing\n",
    "        self.age_fc = nn.Sequential(\n",
    "            nn.Linear(age_dim, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Final layers\n",
    "        self.fc_validity = nn.Linear(512 * 16 * 16 + 256, 1)\n",
    "        self.fc_age_match = nn.Sequential(\n",
    "            nn.Linear(512 * 16 * 16 + 256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def _conv_block(self, in_ch, out_ch, normalize=True):\n",
    "        layers = [nn.Conv2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_ch))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, sketch, age_emb):\n",
    "        x = self.conv1(sketch)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        age_features = self.age_fc(age_emb)\n",
    "        \n",
    "        combined = torch.cat([x, age_features], dim=1)\n",
    "        \n",
    "        validity = self.fc_validity(combined)\n",
    "        age_match = self.fc_age_match(combined)\n",
    "        \n",
    "        return validity, age_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57490d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üß† Perceptual Loss (VGG)\n",
    "# =========================================================\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        try:\n",
    "            vgg = models.vgg16(pretrained=True).features[:16]\n",
    "            self.vgg = vgg.eval()\n",
    "            for param in self.vgg.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.enabled = True\n",
    "        except:\n",
    "            self.enabled = False\n",
    "            print(\"‚ö†Ô∏è VGG model not available, perceptual loss disabled\")\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        if not self.enabled:\n",
    "            return torch.tensor(0.0, device=pred.device)\n",
    "        \n",
    "        pred = (pred + 1) / 2\n",
    "        target = (target + 1) / 2\n",
    "        pred_features = self.vgg(pred)\n",
    "        target_features = self.vgg(target)\n",
    "        return torch.mean(torch.abs(pred_features - target_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6078138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üéì Initialize Models\n",
    "# =========================================================\n",
    "print(\"\\nüèóÔ∏è Building models...\")\n",
    "generator = AgeConditionedGenerator(TEXT_EMB_DIM, AGE_EMB_DIM).to(device)\n",
    "discriminator = AgeAwareDiscriminator(AGE_EMB_DIM).to(device)\n",
    "perceptual_loss_fn = PerceptualLoss().to(device)\n",
    "\n",
    "print(f\"‚úÖ Generator params: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "print(f\"‚úÖ Discriminator params: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "\n",
    "# Optimizers\n",
    "gen_opt = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "disc_opt = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss functions\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "l1_loss = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41638444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# ‚öñÔ∏è Loss Functions (FIXED for MPS)\n",
    "# =========================================================\n",
    "def compute_clip_loss(generated_sketches, text_descriptions):\n",
    "    \"\"\"Compute CLIP similarity loss (with fallback) - MPS COMPATIBLE\"\"\"\n",
    "    if clip_model is None:\n",
    "        return torch.tensor(0.0, device=device, dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        similarities = []\n",
    "        # Convert to numpy on CPU to avoid MPS dtype issues\n",
    "        gen_np = generated_sketches.detach().cpu().float().numpy()  # Ensure float32\n",
    "        gen_np = ((gen_np + 1) * 127.5).astype(np.uint8)\n",
    "        \n",
    "        for i, text in enumerate(text_descriptions):\n",
    "            sketch = gen_np[i].transpose(1, 2, 0)\n",
    "            sim = clip_similarity(text, sketch)\n",
    "            # Ensure similarity is in valid range, then compute loss\n",
    "            sim = np.clip(sim, -1.0, 1.0)\n",
    "            loss = max(0.0, 1.0 - sim)\n",
    "            similarities.append(loss)\n",
    "        \n",
    "        # Create tensor with explicit float32 dtype\n",
    "        return torch.tensor(\n",
    "            np.mean(similarities), \n",
    "            device=device, \n",
    "            dtype=torch.float32,  # Explicit float32\n",
    "            requires_grad=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è CLIP loss error: {e}\")\n",
    "        return torch.tensor(0.0, device=device, dtype=torch.float32)\n",
    "\n",
    "def generator_loss(disc_validity, disc_age_match, gen_sketch, real_sketch, texts):\n",
    "    \"\"\"Combined generator loss - MPS COMPATIBLE\"\"\"\n",
    "    # Adversarial loss\n",
    "    adv_loss = bce_loss(disc_validity, torch.ones_like(disc_validity))\n",
    "    \n",
    "    # Age matching loss\n",
    "    age_loss = bce_loss(disc_age_match, torch.ones_like(disc_age_match))\n",
    "    \n",
    "    # L1 reconstruction loss\n",
    "    l1 = l1_loss(gen_sketch, real_sketch)\n",
    "    \n",
    "    # Perceptual loss\n",
    "    perc = perceptual_loss_fn(gen_sketch, real_sketch)\n",
    "    \n",
    "    # CLIP alignment loss (now MPS-compatible)\n",
    "    clip_loss = compute_clip_loss(gen_sketch, texts)\n",
    "    \n",
    "    # Ensure all losses are float32\n",
    "    adv_loss = adv_loss.float()\n",
    "    age_loss = age_loss.float()\n",
    "    l1 = l1.float()\n",
    "    perc = perc.float()\n",
    "    clip_loss = clip_loss.float()\n",
    "    \n",
    "    # Combined loss with weights\n",
    "    total = (\n",
    "        1.0 * adv_loss +        # Adversarial\n",
    "        LAMBDA_L1 * l1 +        # L1 reconstruction (weight: 100)\n",
    "        10.0 * perc +           # Perceptual\n",
    "        5.0 * clip_loss +       # CLIP alignment\n",
    "        3.0 * age_loss          # Age consistency\n",
    "    )\n",
    "    \n",
    "    return total, adv_loss, l1, perc, clip_loss, age_loss\n",
    "\n",
    "def discriminator_loss(disc_real_val, disc_real_age, disc_fake_val, disc_fake_age):\n",
    "    \"\"\"Discriminator loss - MPS COMPATIBLE\"\"\"\n",
    "    # Real images should be classified as real (1)\n",
    "    real_loss = bce_loss(disc_real_val, torch.ones_like(disc_real_val))\n",
    "    \n",
    "    # Fake images should be classified as fake (0)\n",
    "    fake_loss = bce_loss(disc_fake_val, torch.zeros_like(disc_fake_val))\n",
    "    \n",
    "    # Combined validity loss\n",
    "    validity_loss = 0.5 * (real_loss + fake_loss)\n",
    "    \n",
    "    # Age matching loss for real images\n",
    "    age_loss = bce_loss(disc_real_age, torch.ones_like(disc_real_age))\n",
    "    \n",
    "    return (validity_loss + age_loss).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55cf3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üì¶ Create DataLoader\n",
    "# =========================================================\n",
    "dataset = SketchDataset(train_A, train_B, desc, estimated_ages)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset size: {len(dataset)}\")\n",
    "print(f\"‚úÖ Batches per epoch: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================================================\n",
    "# üì¶ Create DataLoader\n",
    "# =========================================================\n",
    "dataset = SketchDataset(train_A, train_B, desc, estimated_ages)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0,  # Changed to 0 for macOS\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset size: {len(dataset)}\")\n",
    "print(f\"‚úÖ Batches per epoch: {len(dataloader)}\")\n",
    "\n",
    "# =========================================================\n",
    "# üöÇ Training Loop\n",
    "# =========================================================\n",
    "if __name__ == '__main__':  # ‚Üê Add this guard\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ STARTING TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ... rest of your training code\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training history for plotting\n",
    "history = {\n",
    "    'g_loss': [],\n",
    "    'd_loss': [],\n",
    "    'g_adv_loss': [],\n",
    "    'g_l1_loss': [],\n",
    "    'g_perceptual_loss': [],\n",
    "    'g_clip_loss': [],\n",
    "    'g_age_loss': []\n",
    "}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    epoch_metrics = {\n",
    "        'adv': [], 'l1': [], 'perc': [], 'clip': [], 'age': []\n",
    "    }\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch_idx, (photos, sketches, texts, ages, filenames) in enumerate(progress_bar):\n",
    "        batch_size = photos.size(0)\n",
    "        photos = photos.to(device)\n",
    "        sketches = sketches.to(device)\n",
    "        \n",
    "        # Get text embeddings\n",
    "        try:\n",
    "            text_embs = torch.stack([\n",
    "                torch.FloatTensor(clip_text_embedding(text)) \n",
    "                for text in texts\n",
    "            ]).to(device)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Text embedding error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Get age embeddings\n",
    "        age_embs = torch.stack([encode_age(age.item()) for age in ages]).to(device)\n",
    "        \n",
    "        # =====================================\n",
    "        # Train Discriminator\n",
    "        # =====================================\n",
    "        disc_opt.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            gen_sketches = generator(text_embs, age_embs)\n",
    "        \n",
    "        disc_real_val, disc_real_age = discriminator(sketches, age_embs)\n",
    "        disc_fake_val, disc_fake_age = discriminator(gen_sketches.detach(), age_embs)\n",
    "        \n",
    "        d_loss = discriminator_loss(disc_real_val, disc_real_age, disc_fake_val, disc_fake_age)\n",
    "        d_loss.backward()\n",
    "        disc_opt.step()\n",
    "        \n",
    "        # =====================================\n",
    "        # Train Generator\n",
    "        # =====================================\n",
    "        gen_opt.zero_grad()\n",
    "        \n",
    "        gen_sketches = generator(text_embs, age_embs)\n",
    "        disc_fake_val, disc_fake_age = discriminator(gen_sketches, age_embs)\n",
    "        \n",
    "        g_loss, adv, l1, perc, clip_l, age_l = generator_loss(\n",
    "            disc_fake_val, disc_fake_age, gen_sketches, sketches, texts\n",
    "        )\n",
    "        g_loss.backward()\n",
    "        gen_opt.step()\n",
    "        \n",
    "        # Record losses\n",
    "        g_losses.append(g_loss.item())\n",
    "        d_losses.append(d_loss.item())\n",
    "        epoch_metrics['adv'].append(adv.item())\n",
    "        epoch_metrics['l1'].append(l1.item())\n",
    "        epoch_metrics['perc'].append(perc.item())\n",
    "        epoch_metrics['clip'].append(clip_l.item())\n",
    "        epoch_metrics['age'].append(age_l.item())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'G': f'{np.mean(g_losses):.3f}',\n",
    "            'D': f'{np.mean(d_losses):.3f}',\n",
    "            'L1': f'{np.mean(epoch_metrics[\"l1\"]):.3f}'\n",
    "        })\n",
    "    \n",
    "    # Record epoch history\n",
    "    history['g_loss'].append(np.mean(g_losses))\n",
    "    history['d_loss'].append(np.mean(d_losses))\n",
    "    history['g_adv_loss'].append(np.mean(epoch_metrics['adv']))\n",
    "    history['g_l1_loss'].append(np.mean(epoch_metrics['l1']))\n",
    "    history['g_perceptual_loss'].append(np.mean(epoch_metrics['perc']))\n",
    "    history['g_clip_loss'].append(np.mean(epoch_metrics['clip']))\n",
    "    history['g_age_loss'].append(np.mean(epoch_metrics['age']))\n",
    "    \n",
    "    # Save checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = f\"./checkpoints/checkpoint_epoch_{epoch+1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator': generator.state_dict(),\n",
    "            'discriminator': discriminator.state_dict(),\n",
    "            'gen_opt': gen_opt.state_dict(),\n",
    "            'disc_opt': disc_opt.state_dict(),\n",
    "            'history': history\n",
    "        }, checkpoint_path)\n",
    "        print(f\"\\nüíæ Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    eta = (elapsed / (epoch + 1)) * (EPOCHS - epoch - 1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Epoch {epoch+1}/{EPOCHS} completed:\")\n",
    "    print(f\"   Generator Loss: {np.mean(g_losses):.4f}\")\n",
    "    print(f\"   Discriminator Loss: {np.mean(d_losses):.4f}\")\n",
    "    print(f\"   L1 Loss: {np.mean(epoch_metrics['l1']):.4f}\")\n",
    "    print(f\"   Perceptual Loss: {np.mean(epoch_metrics['perc']):.4f}\")\n",
    "    print(f\"   CLIP Loss: {np.mean(epoch_metrics['clip']):.4f}\")\n",
    "    print(f\"   Age Loss: {np.mean(epoch_metrics['age']):.4f}\")\n",
    "    print(f\"   ‚è± Time: {elapsed/60:.1f}m | ETA: {eta/60:.1f}m\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ TRAINING COMPLETE! Total time: {total_time/60:.1f} minutes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save final models\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "torch.save(generator.state_dict(), \"./models/generator_final.pth\")\n",
    "torch.save(discriminator.state_dict(), \"./models/discriminator_final.pth\")\n",
    "torch.save(history, \"./models/training_history.pth\")\n",
    "\n",
    "print(\"\\nüíæ Final models saved:\")\n",
    "print(\"   Generator: ./models/generator_final.pth\")\n",
    "print(\"   Discriminator: ./models/discriminator_final.pth\")\n",
    "print(\"   History: ./models/training_history.pth\")\n",
    "\n",
    "# =========================================================\n",
    "# üìä Plot Training History\n",
    "# =========================================================\n",
    "print(\"\\nüìä Plotting training history...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Training Progress', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Generator and Discriminator Loss\n",
    "axes[0, 0].plot(history['g_loss'], label='Generator', color='blue', linewidth=2)\n",
    "axes[0, 0].plot(history['d_loss'], label='Discriminator', color='red', linewidth=2)\n",
    "axes[0, 0].set_title('Generator vs Discriminator Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Adversarial Loss\n",
    "axes[0, 1].plot(history['g_adv_loss'], color='purple', linewidth=2)\n",
    "axes[0, 1].set_title('Adversarial Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# L1 Loss\n",
    "axes[0, 2].plot(history['g_l1_loss'], color='green', linewidth=2)\n",
    "axes[0, 2].set_title('L1 Reconstruction Loss')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('Loss')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Perceptual Loss\n",
    "axes[1, 0].plot(history['g_perceptual_loss'], color='orange', linewidth=2)\n",
    "axes[1, 0].set_title('Perceptual Loss')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# CLIP Loss\n",
    "axes[1, 1].plot(history['g_clip_loss'], color='cyan', linewidth=2)\n",
    "axes[1, 1].set_title('CLIP Alignment Loss')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Age Loss\n",
    "axes[1, 2].plot(history['g_age_loss'], color='magenta', linewidth=2)\n",
    "axes[1, 2].set_title('Age Consistency Loss')\n",
    "axes[1, 2].set_xlabel('Epoch')\n",
    "axes[1, 2].set_ylabel('Loss')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./models/training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Training history plot saved: ./models/training_history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================================================\n",
    "# üé® FIXED GENERATION FUNCTIONS\n",
    "# =========================================================\n",
    "\n",
    "def create_truly_unique_latent(description, seed=None):\n",
    "    \"\"\"\n",
    "    Create a TRULY unique latent vector that varies significantly \n",
    "    between different descriptions.\n",
    "    \n",
    "    The key: Use random noise INSTEAD of deterministic embeddings\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        # Create unique seed from description\n",
    "        seed = abs(hash(description)) % (2**32)\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate random latent vector (THIS is what creates different faces)\n",
    "    latent = np.random.randn(TEXT_EMB_DIM).astype(np.float32)\n",
    "    \n",
    "    # Optional: Mix in some semantic features from text (10% max)\n",
    "    try:\n",
    "        text_features = clip_text_embedding(description)\n",
    "        # 90% random noise + 10% text features\n",
    "        latent = 0.9 * latent + 0.1 * text_features\n",
    "    except:\n",
    "        pass  # Use pure random if CLIP fails\n",
    "    \n",
    "    # Normalize\n",
    "    latent = latent / np.linalg.norm(latent)\n",
    "    \n",
    "    return latent, seed\n",
    "\n",
    "\n",
    "def generate_sketch_at_age_fixed(description, target_age, variation_seed=None):\n",
    "    \"\"\"\n",
    "    FIXED: Generate different faces for different descriptions\n",
    "    \"\"\"\n",
    "    # Create UNIQUE latent vector (this is what makes faces different)\n",
    "    latent_vector, used_seed = create_truly_unique_latent(description, variation_seed)\n",
    "    \n",
    "    print(f\"üîç Generating: '{description[:50]}...' at age {target_age} (seed: {used_seed})\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Convert to tensor\n",
    "        text_emb = torch.FloatTensor(latent_vector).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get age embedding\n",
    "        age_emb = encode_age(target_age).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate\n",
    "        generated = generator(text_emb, age_emb)\n",
    "        sketch = generated[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        sketch = ((sketch + 1.0) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "    \n",
    "    return sketch\n",
    "\n",
    "\n",
    "def generate_age_progression_fixed(description, start_age=25, end_age=65, steps=5):\n",
    "    \"\"\"\n",
    "    FIXED: Generate age progression maintaining identity\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚è∞ Age progression: '{description}' ({start_age} ‚Üí {end_age})\")\n",
    "    \n",
    "    ages = np.linspace(start_age, end_age, steps, dtype=int)\n",
    "    sketches = []\n",
    "    \n",
    "    # CRITICAL: Use SAME seed for all ages to maintain identity\n",
    "    base_seed = abs(hash(description)) % (2**32)\n",
    "    print(f\"   Using seed: {base_seed}\")\n",
    "    \n",
    "    for age in ages:\n",
    "        sketch = generate_sketch_at_age_fixed(description, int(age), variation_seed=base_seed)\n",
    "        sketches.append((int(age), sketch))\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, len(sketches), figsize=(4*len(sketches), 4))\n",
    "    if len(sketches) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (age, sketch) in zip(axes, sketches):\n",
    "        ax.imshow(sketch)\n",
    "        ax.set_title(f\"Age {age}\", fontsize=12, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Age Progression: {description}\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    safe_desc = \"\".join(c for c in description[:30] if c.isalnum() or c in (' ', '_')).strip()\n",
    "    filename = f\"./age_progression_results/fixed_progression_{safe_desc}_{start_age}_{end_age}.png\"\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Saved: {filename}\")\n",
    "    \n",
    "    return sketches\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# üß™ TEST SUITE - Verify Different Faces\n",
    "# =========================================================\n",
    "\n",
    "def test_different_descriptions_create_different_faces():\n",
    "    \"\"\"\n",
    "    This test MUST show 4 completely different faces!\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üî¨ TEST: Different Descriptions ‚Üí Different Faces\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    descriptions = [\n",
    "        \"woman with long blonde hair\",\n",
    "        \"man with short dark hair and beard\",\n",
    "        \"elderly person with glasses\",\n",
    "        \"young person with curly hair\"\n",
    "    ]\n",
    "    \n",
    "    age = 35\n",
    "    fig, axes = plt.subplots(1, len(descriptions), figsize=(5*len(descriptions), 5))\n",
    "    \n",
    "    for i, desc in enumerate(descriptions):\n",
    "        sketch = generate_sketch_at_age_fixed(desc, age)\n",
    "        axes[i].imshow(sketch)\n",
    "        axes[i].set_title(f\"{desc}\\nAge {age}\", fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"‚úÖ These Should Be 4 DIFFERENT Faces!\", \n",
    "                 fontsize=14, fontweight='bold', color='green')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./age_progression_results/test_different_faces.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Test complete! Check that all 4 faces look different.\")\n",
    "\n",
    "\n",
    "def test_same_description_maintains_identity():\n",
    "    \"\"\"\n",
    "    This test MUST show the SAME person aging!\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üî¨ TEST: Same Description ‚Üí Same Identity Across Ages\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    description = \"woman with long hair\"\n",
    "    ages = [25, 35, 45, 55, 65]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(ages), figsize=(4*len(ages), 4))\n",
    "    \n",
    "    # Use consistent seed\n",
    "    seed = abs(hash(description)) % (2**32)\n",
    "    \n",
    "    for i, age in enumerate(ages):\n",
    "        sketch = generate_sketch_at_age_fixed(description, age, variation_seed=seed)\n",
    "        axes[i].imshow(sketch)\n",
    "        axes[i].set_title(f\"Age {age}\", fontsize=12, fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"‚úÖ These Should Be the SAME Person Aging!\", \n",
    "                 fontsize=14, fontweight='bold', color='green')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./age_progression_results/test_same_identity.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Test complete! Check that all faces look like the same person.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9548dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üöÄ RUN TESTS\n",
    "# =========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üé® RUNNING FIXED TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Different descriptions\n",
    "try:\n",
    "    test_different_descriptions_create_different_faces()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test 2: Same identity\n",
    "try:\n",
    "    test_same_description_maintains_identity()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test 3: Multiple age progressions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî¨ TEST: Multiple Different Age Progressions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "descriptions = [\n",
    "    \"a young woman with short hair\",\n",
    "    \"a man with a mustache\",\n",
    "    \"a person with glasses\"\n",
    "]\n",
    "\n",
    "for desc in descriptions:\n",
    "    try:\n",
    "        generate_age_progression_fixed(desc, 25, 65, 5)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed for '{desc}': {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ALL TESTS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCheck ./age_progression_results/ for results:\")\n",
    "print(\"  - test_different_faces.png (should show 4 DIFFERENT people)\")\n",
    "print(\"  - test_same_identity.png (should show 1 person aging)\")\n",
    "print(\"  - fixed_progression_*.png (age progression sequences)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
